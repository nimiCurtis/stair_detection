hydra:
  run:
    dir: ./config/saved_configs/stair_detection_config/${now:%Y-%m-%d}_${now:%H-%M-%S}

# model configuration
model:
  model: ./models/yolov8/yolov8n.pt
  task: detect

data: 
  root: datasets
  project: single_stair_detection
  version: v7i
  model: yolov8

# train configuration
train:
  epochs: 200 # number of epochs to train for
  patience: 80 # epochs to wait for no observable improvement for early stopping of training
  batch: 64 # number of images per batch (-1 for AutoBatch)
  imgsz: 
  - 320 # size of input images as integer
  - 320
  save: True # save train checkpoints and predict results
  device: cuda # device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu
  pretrained: True # (bool or str) whether to use a pretrained model (bool) or a model to load weights from (str)
  amp: False # Automatic Mixed Precision (AMP) training, choices=[True, False]
  close_mosaic: 80 # (int) disable mosaic augmentation for final epochs (0 to disable)
  project: experiments_${data.project} # project name
  name: ${data.model}_${data.version}_imgsz${train.imgsz[0]}_${now:%Y-%m-%d}_${now:%H-%M-%S} # experiment name

  save_period: -1 # Save checkpoint every x epochs (disabled if < 1)
  cache: False # True/ram, disk or False. Use cache for data loading
  workers: 8 # number of worker threads for data loading (per RANK if DDP)
  exist_ok: False # whether to overwrite existing experiment
  optimizer: 'auto' # optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]
  verbose: False # whether to print verbose output
  seed: 0 # random seed for reproducibility
  deterministic: True # whether to enable deterministic mode
  single_cls: False # train multi-class data as single-class
  rect: False # rectangular training with each batch collated for minimum padding
  cos_lr: False # use cosine learning rate scheduler
  resume: False # resume training from last checkpoint
  fraction: 1.0 # dataset fraction to train on (default is 1.0, all images in train set)
  profile: False # profile ONNX and TensorRT speeds during training for loggers
  freeze: None # (int or list, optional) freeze first n layers, or freeze list of layer indices during training
  lr0: 0.01 # initial learning rate (i.e. SGD=1E-2, Adam=1E-3)
  lrf: 0.01 # final learning rate (lr0 * lrf)
  momentum: 0.937 # SGD momentum/Adam beta1
  weight_decay: 0.0005 # optimizer weight decay 5e-4
  warmup_epochs: 3.0 # warmup epochs (fractions ok)
  warmup_momentum: 0.8 # warmup initial momentum
  warmup_bias_lr: 0.1 # warmup initial bias lr
  box: 7.5 # box loss gain
  cls: 0.5 # cls loss gain (scale with pixels)
  dfl: 1.5 # dfl loss gain
  pose: 12.0 # pose loss gain (pose-only)
  kobj: 2.0 # keypoint obj loss gain (pose-only)
  label_smoothing: 0.0 # label smoothing (fraction)
  nbs: 64 # nominal batch size
  overlap_mask: True # masks should overlap during training (segment train only)
  mask_ratio: 4 # mask downsample ratio (segment train only)
  dropout: 0.0 # use dropout regularization (classify train only)
  val: True # validate/test during training

  ## Augmentations:
  
  # hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
  # hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
  # hsv_v: 0.4  # image HSV-Value augmentation (fraction)
  # degrees: 0.0  # image rotation (+/- deg)
  # translate: 0.1  # image translation (+/- fraction)
  # scale: 0.2  # image scale (+/- gain)
  # shear: 0.2  # image shear (+/- deg) from -0.5 to 0.5
  # perspective: 0.1  # image perspective (+/- fraction), range 0-0.001
  # flipud: 0.7  # image flip up-down (probability)
  # fliplr: 0.5  # image flip left-right (probability)
  # mosaic: 0.3  # image mosaic (probability)
  # mixup: 0.1  # image mixup (probability)